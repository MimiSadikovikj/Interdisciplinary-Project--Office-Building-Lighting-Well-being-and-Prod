{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "218d5847",
   "metadata": {},
   "source": [
    "### Annotation Analysis - Label: Negative Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2769b42f",
   "metadata": {},
   "source": [
    "* Number of papers annotated: 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0586fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39c44595",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path containing the annotation.json files and the targeted label\n",
    "directory_path = './data'  # Update this to your actual directory path\n",
    "targeted_label = 'NEGATIVE WORD'\n",
    "random.seed(11824061)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52b7b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json_file(file_path):\n",
    "    \"\"\"Function to read json files\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "997925e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words_from_category(text, entities, category):\n",
    "    \"\"\"Function to extract all words which are tagged with a given category\"\"\"\n",
    "    words = []\n",
    "    for start, end, cat in entities:\n",
    "        if cat == category:\n",
    "            words.append(text[start:end])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b7c4479",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_category_words_all(data):\n",
    "    \"\"\"Iterate through annotations and extract words for each category\"\"\"\n",
    "    category_words_dict = {}\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation is None:\n",
    "            continue\n",
    "        text = annotation[0]\n",
    "        entities = annotation[1]['entities']\n",
    "        for category in set([entity[2] for entity in entities]): \n",
    "            words = extract_words_from_category(text, entities, category)\n",
    "            if category in category_words_dict:\n",
    "                category_words_dict[category].extend(words)\n",
    "            else:\n",
    "                category_words_dict[category] = words\n",
    "    return category_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1826f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all annotations from the specified directory\n",
    "annotations = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        data = read_json_file(file_path)\n",
    "        annotations.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75b8a562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove None entries from annotations\n",
    "def remove_null_entries(annotations):\n",
    "    for paper in annotations:\n",
    "        paper['annotations'] = [annotation for annotation in paper['annotations'] if annotation is not None]\n",
    "remove_null_entries(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58737608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean keywords\n",
    "def clean_word(word):\n",
    "    word = re.sub(r'-[\\r\\n]+', '', word)\n",
    "    word = word.replace('\\n', '').replace('\\r', '') \n",
    "    word = word.rstrip('.')\n",
    "    word = word.strip()\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209d2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove entities from annotations\n",
    "def remove_entities(annotations):\n",
    "    for paper in annotations:\n",
    "        for annotation in paper['annotations']:\n",
    "            annotation[1]['entities'] = []\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac73a646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean text before keyword search\n",
    "def clean_text_before_keyword_search(text):\n",
    "    replacements = [\n",
    "        (\"-\\r\\n\", \"   \"),  \n",
    "        (\"\\r\\n\", \"  \"),  \n",
    "        (\"\\r\", \" \"),  \n",
    "        (\"\\n\", \" \"),  \n",
    "    ]\n",
    "    for old, new in replacements:\n",
    "        text = text.replace(old, new)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ad04607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find keyword positions in the text\n",
    "def find_keyword_positions(keywordlist, text, label):\n",
    "    annotations = set()\n",
    "    for keyword in keywordlist:\n",
    "        pattern = re.compile(r'\\b' + r'\\s*'.join(re.escape(char) for char in keyword) + r'\\b', re.IGNORECASE)\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            if text[end:end + 1] == \".\":\n",
    "                end += 1\n",
    "            annotations.add((start, end, label))\n",
    "    return [list(annotation) for annotation in annotations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1dcb0d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_with_keyword_list(data, keyword_list, entity_type=\"NEGATIVE WORD\"):\n",
    "    for paper in data:\n",
    "        for annotation in paper['annotations']:\n",
    "            text = annotation[0]\n",
    "            annotation[1]['entities'] = find_keyword_positions(keyword_list, text, entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bde787d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter annotations by label\n",
    "def filter_annotations(data, label):\n",
    "    for paper in data:\n",
    "        for annotation in paper['annotations']:\n",
    "            filtered_entities = [entity for entity in annotation[1]['entities'] if entity[2] == label]\n",
    "            annotation[1]['entities'] = filtered_entities if filtered_entities else []\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d360441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create new structure for annotations\n",
    "def create_new_structure_for_annotations(data):\n",
    "    output_list = []\n",
    "    for paper in data:\n",
    "        for paragraph in paper[\"annotations\"]:\n",
    "            text_dict = {\"text\": paragraph[0], \"entities\": paragraph[1][\"entities\"]}\n",
    "            output_list.append(text_dict)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f102aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    def extract_entities(data):\n",
    "        entities = set()\n",
    "        for key, item in enumerate(data):\n",
    "            for entity in item[\"entities\"]:\n",
    "                start, end, label = entity\n",
    "                entities.add((key, start, end, label))\n",
    "        return entities\n",
    "\n",
    "    pred_entities = extract_entities(predictions)\n",
    "    gt_entities = extract_entities(ground_truth)\n",
    "\n",
    "    true_positives = len(pred_entities & gt_entities)\n",
    "    false_positives = len(pred_entities - gt_entities)\n",
    "    false_negatives = len(gt_entities - pred_entities)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = true_positives / (true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fcdda2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.2848199515323574\n",
      "Recall: 0.5578833730252097\n",
      "F1 Score: 0.3757449458428427\n",
      "Accuracy: 0.24126806959707858\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=11824061)\n",
    "precisions, recalls, f1_scores, accuracies = [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(annotations):\n",
    "    annotations_train = [annotations[i] for i in train_index]\n",
    "    annotations_test = [annotations[i] for i in test_index]\n",
    "    \n",
    "    # Create a keyword list from training data annotations\n",
    "    keyword_list = []\n",
    "    for annotation in annotations_train:\n",
    "        category_words_dict = extract_category_words_all(annotation)\n",
    "        keyword_list.extend(category_words_dict.get(targeted_label, []))\n",
    "    \n",
    "    keyword_list = [clean_word(word) for word in keyword_list]\n",
    "    keyword_list = list(set(keyword_list))\n",
    "    \n",
    "    annotations_ground_truth = copy.deepcopy(annotations_test)\n",
    "    annotations_pred = remove_entities(copy.deepcopy(annotations_test))\n",
    "    \n",
    "    annotate_with_keyword_list(annotations_pred, keyword_list, targeted_label)\n",
    "    \n",
    "    annotations_ground_truth_filtered = filter_annotations(annotations_ground_truth, targeted_label)\n",
    "    \n",
    "    ground_truth = create_new_structure_for_annotations(annotations_ground_truth_filtered)\n",
    "    predictions = create_new_structure_for_annotations(annotations_pred)\n",
    "    \n",
    "    precision, recall, f1, accuracy = calculate_metrics(predictions, ground_truth)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f'Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(f'Accuracy: {sum(accuracies) / len(accuracies)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d100edf",
   "metadata": {},
   "source": [
    "### Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37e9a72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mimis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mimis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.10562318434684195\n",
      "Recall: 0.561773801564279\n",
      "F1 Score: 0.1773787961126388\n",
      "Accuracy: 0.09832872722694426\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import random\n",
    "import nltk\n",
    "from sklearn.model_selection import KFold\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Define the directory path containing the annotation.json files and the targeted label\n",
    "directory_path = './data'  # Update this to your actual directory path\n",
    "targeted_label = 'NEGATIVE WORD'\n",
    "random.seed(11824061)\n",
    "\n",
    "def read_json_file(file_path):\n",
    "    \"\"\"Function to read json files\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def extract_words_from_category(text, entities, category):\n",
    "    \"\"\"Function to extract all words which are tagged with a given category\"\"\"\n",
    "    words = []\n",
    "    for start, end, cat in entities:\n",
    "        if cat == category:\n",
    "            words.append(text[start:end])\n",
    "    return words\n",
    "\n",
    "def extract_category_words_all(data):\n",
    "    \"\"\"Iterate through annotations and extract words for each category\"\"\"\n",
    "    category_words_dict = {}\n",
    "    for annotation in data['annotations']:\n",
    "        if annotation is None:\n",
    "            continue\n",
    "        text = annotation[0]\n",
    "        entities = annotation[1]['entities']\n",
    "        for category in set([entity[2] for entity in entities]): \n",
    "            words = extract_words_from_category(text, entities, category)\n",
    "            if category in category_words_dict:\n",
    "                category_words_dict[category].extend(words)\n",
    "            else:\n",
    "                category_words_dict[category] = words\n",
    "    return category_words_dict\n",
    "\n",
    "# Read all annotations from the specified directory\n",
    "annotations = []\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        data = read_json_file(file_path)\n",
    "        annotations.append(data)\n",
    "\n",
    "# Remove None entries from annotations\n",
    "def remove_null_entries(annotations):\n",
    "    for paper in annotations:\n",
    "        paper['annotations'] = [annotation for annotation in paper['annotations'] if annotation is not None]\n",
    "remove_null_entries(annotations)\n",
    "\n",
    "# Function to clean keywords\n",
    "def clean_word(word):\n",
    "    word = re.sub(r'-[\\r\\n]+', '', word)\n",
    "    word = word.replace('\\n', '').replace('\\r', '') \n",
    "    word = word.rstrip('.')\n",
    "    word = word.strip()\n",
    "    return word\n",
    "\n",
    "# Function to expand keywords using WordNet\n",
    "def expand_keywords_with_wordnet(keyword_list):\n",
    "    expanded_keywords = set(keyword_list)\n",
    "    for word in keyword_list:\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_keywords.add(lemma.name())\n",
    "    return list(expanded_keywords)\n",
    "\n",
    "# Function to remove entities from annotations\n",
    "def remove_entities(annotations):\n",
    "    for paper in annotations:\n",
    "        for annotation in paper['annotations']:\n",
    "            annotation[1]['entities'] = []\n",
    "    return annotations\n",
    "\n",
    "# Function to clean text before keyword search\n",
    "def clean_text_before_keyword_search(text):\n",
    "    replacements = [\n",
    "        (\"-\\r\\n\", \"   \"),  \n",
    "        (\"\\r\\n\", \"  \"),  \n",
    "        (\"\\r\", \" \"),  \n",
    "        (\"\\n\", \" \"),  \n",
    "    ]\n",
    "    for old, new in replacements:\n",
    "        text = text.replace(old, new)\n",
    "    return text\n",
    "\n",
    "# Function to find keyword positions in the text\n",
    "def find_keyword_positions(keywordlist, text, label):\n",
    "    annotations = set()\n",
    "    for keyword in keywordlist:\n",
    "        pattern = re.compile(r'\\b' + re.escape(keyword) + r'\\b', re.IGNORECASE)\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            annotations.add((start, end, label))\n",
    "    return [list(annotation) for annotation in annotations]\n",
    "\n",
    "def annotate_with_keyword_list(data, keyword_list, entity_type=\"NEGATIVE WORD\"):\n",
    "    for paper in data:\n",
    "        for annotation in paper['annotations']:\n",
    "            text = clean_text_before_keyword_search(annotation[0])\n",
    "            annotation[1]['entities'] = find_keyword_positions(keyword_list, text, entity_type)\n",
    "\n",
    "# Function to filter annotations by label\n",
    "def filter_annotations(data, label):\n",
    "    for paper in data:\n",
    "        for annotation in paper['annotations']:\n",
    "            filtered_entities = [entity for entity in annotation[1]['entities'] if entity[2] == label]\n",
    "            annotation[1]['entities'] = filtered_entities if filtered_entities else []\n",
    "    return data\n",
    "\n",
    "# Function to create new structure for annotations\n",
    "def create_new_structure_for_annotations(data):\n",
    "    output_list = []\n",
    "    for paper in data:\n",
    "        for paragraph in paper[\"annotations\"]:\n",
    "            text_dict = {\"text\": paragraph[0], \"entities\": paragraph[1][\"entities\"]}\n",
    "            output_list.append(text_dict)\n",
    "    return output_list\n",
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(predictions, ground_truth):\n",
    "    def extract_entities(data):\n",
    "        entities = set()\n",
    "        for key, item in enumerate(data):\n",
    "            for entity in item[\"entities\"]:\n",
    "                start, end, label = entity[:3]\n",
    "                entities.add((key, start, end, label))\n",
    "        return entities\n",
    "\n",
    "    pred_entities = extract_entities(predictions)\n",
    "    gt_entities = extract_entities(ground_truth)\n",
    "\n",
    "    true_positives = len(pred_entities & gt_entities)\n",
    "    false_positives = len(pred_entities - gt_entities)\n",
    "    false_negatives = len(gt_entities - pred_entities)\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    accuracy = true_positives / (true_positives + false_positives + false_negatives) if (true_positives + false_positives + false_negatives) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score, accuracy\n",
    "\n",
    "# Perform cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=11824061)\n",
    "precisions, recalls, f1_scores, accuracies = [], [], [], []\n",
    "\n",
    "for train_index, test_index in kf.split(annotations):\n",
    "    annotations_train = [annotations[i] for i in train_index]\n",
    "    annotations_test = [annotations[i] for i in test_index]\n",
    "    \n",
    "    # Create a keyword list from training data annotations\n",
    "    keyword_list = []\n",
    "    for annotation in annotations_train:\n",
    "        category_words_dict = extract_category_words_all(annotation)\n",
    "        keyword_list.extend(category_words_dict.get(targeted_label, []))\n",
    "    \n",
    "    keyword_list = [clean_word(word) for word in keyword_list]\n",
    "    keyword_list = list(set(keyword_list))\n",
    "    \n",
    "    # Expand keyword list with WordNet\n",
    "    keyword_list = expand_keywords_with_wordnet(keyword_list)\n",
    "    \n",
    "    annotations_ground_truth = copy.deepcopy(annotations_test)\n",
    "    annotations_pred = remove_entities(copy.deepcopy(annotations_test))\n",
    "    \n",
    "    annotate_with_keyword_list(annotations_pred, keyword_list, targeted_label)\n",
    "    \n",
    "    annotations_ground_truth_filtered = filter_annotations(annotations_ground_truth, targeted_label)\n",
    "    \n",
    "    ground_truth = create_new_structure_for_annotations(annotations_ground_truth_filtered)\n",
    "    predictions = create_new_structure_for_annotations(annotations_pred)\n",
    "    \n",
    "    precision, recall, f1, accuracy = calculate_metrics(predictions, ground_truth)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "print(f'Precision: {sum(precisions) / len(precisions)}')\n",
    "print(f'Recall: {sum(recalls) / len(recalls)}')\n",
    "print(f'F1 Score: {sum(f1_scores) / len(f1_scores)}')\n",
    "print(f'Accuracy: {sum(accuracies) / len(accuracies)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a309838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
